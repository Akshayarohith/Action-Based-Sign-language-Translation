# Action-Based-Sign-language-Translation
Real-Time Action Based Sign Language is a language that includes gestures with bodily movements made with the hands including facial expressions and postures. It is mainly used by people who are deaf and dumb. There are many different sign languages such as British, Indian and American sign languages. People with disabilities like deaf and dumb use sign language as a tool to express their emotions and thoughts to common people around them. Yet the general public finds it hard to understand the sign and therefore such a trained system like sign language recognition is required during medical and legal appointments, educational and training sessions and for the global meetings being held. A few years ago, there has been an increase in demand for such systems which are formed as video remote human interpreters using high-speed internet connectivity which provided an easy way to translate the sign language that has been used and benefited from yet had a various number of limitations.
To overcome this, we use a Long Short-Term Memory (LSTM) model to detect the actions in sign language. A neural network of six layers is constructed using LSTM deep learning model in which three are LSTM layers and the other three are Dense layers. The dataset we use contains the actions as a specific number of sequences stored as frames which are captured using OpenCV with an interval of time.
